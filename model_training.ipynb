{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11007158,"sourceType":"datasetVersion","datasetId":6852530}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install torch tqdm -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:16:25.410002Z","iopub.execute_input":"2025-03-15T09:16:25.410178Z","iopub.status.idle":"2025-03-15T09:16:29.860642Z","shell.execute_reply.started":"2025-03-15T09:16:25.410159Z","shell.execute_reply":"2025-03-15T09:16:29.859389Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:17:44.483755Z","iopub.execute_input":"2025-03-15T09:17:44.484064Z","iopub.status.idle":"2025-03-15T09:17:44.488078Z","shell.execute_reply.started":"2025-03-15T09:17:44.484043Z","shell.execute_reply":"2025-03-15T09:17:44.487221Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"with open(\"/kaggle/input/ukr-poems/poems.txt\", \"r\") as file:\n    poetry = \"\".join(file.readlines())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:16:36.254208Z","iopub.execute_input":"2025-03-15T09:16:36.254635Z","iopub.status.idle":"2025-03-15T09:16:36.390296Z","shell.execute_reply.started":"2025-03-15T09:16:36.254606Z","shell.execute_reply":"2025-03-15T09:16:36.389611Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"poetry = poetry.lower()\npoetry[:100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:16:37.621406Z","iopub.execute_input":"2025-03-15T09:16:37.621681Z","iopub.status.idle":"2025-03-15T09:16:37.642585Z","shell.execute_reply.started":"2025-03-15T09:16:37.621661Z","shell.execute_reply":"2025-03-15T09:16:37.641817Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'<і смеркає, і світає,\\nдень божий минає,\\nі знову люд потомлений\\nі все спочиває.\\nтілько я, мов окаянни'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import string\nukrainian_alphabet = \"абвгґдеєжзиіїйклмнопрстуфхцчшщьюя\"\n\npunctuation = \".,-:;?!…\"\n\nspecial_tokens = \"<> \\n\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:16:39.483669Z","iopub.execute_input":"2025-03-15T09:16:39.484025Z","iopub.status.idle":"2025-03-15T09:16:39.488014Z","shell.execute_reply.started":"2025-03-15T09:16:39.483998Z","shell.execute_reply":"2025-03-15T09:16:39.487014Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"chars = tuple(set(ukrainian_alphabet + punctuation + special_tokens))\n\nchar_to_idx = {ch: idx for idx, ch in enumerate(chars)}\nidx_to_char = {idx: ch for idx, ch in enumerate(chars)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:16:40.252774Z","iopub.execute_input":"2025-03-15T09:16:40.253081Z","iopub.status.idle":"2025-03-15T09:16:40.257135Z","shell.execute_reply.started":"2025-03-15T09:16:40.253056Z","shell.execute_reply":"2025-03-15T09:16:40.256344Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def clean_text(text):\n    \"\"\"Removes characters not in the 'chars' set.\"\"\"\n    text = \"\".join(ch if ch in chars else \"\" for ch in text)\n    text = text.replace(\"><\", \"\\n\")\n    text = text[1:-1]\n    return text\n\n\ncleaned_poetry = clean_text(poetry)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:16:41.762252Z","iopub.execute_input":"2025-03-15T09:16:41.762554Z","iopub.status.idle":"2025-03-15T09:16:42.710208Z","shell.execute_reply.started":"2025-03-15T09:16:41.762526Z","shell.execute_reply":"2025-03-15T09:16:42.709531Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"encoded_poetry = [char_to_idx[c] for c in cleaned_poetry]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:16:42.711088Z","iopub.execute_input":"2025-03-15T09:16:42.711280Z","iopub.status.idle":"2025-03-15T09:16:42.882926Z","shell.execute_reply.started":"2025-03-15T09:16:42.711263Z","shell.execute_reply":"2025-03-15T09:16:42.882102Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Defining method to make mini-batches for training\ndef get_batches(arr, batch_size, seq_length):\n    # determine the flattened batch size, i.e. sequence length times batch size\n    batch_size_total = batch_size * seq_length\n    # total number of batches we can make\n    n_batches = len(arr)//batch_size_total\n\n    # Keep only enough characters to make full batches\n    arr = arr[:n_batches * batch_size_total]\n    # Reshape into batch_size rows\n    arr = arr.reshape((batch_size, -1))\n\n    # iterate through the array, one sequence at a time\n    for n in range(0, arr.shape[1], seq_length):\n        # The features\n        x = arr[:, n:n+seq_length]\n        # The targets\n        y = np.zeros_like(x)\n        try:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n        except IndexError:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n        yield x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:16:43.780019Z","iopub.execute_input":"2025-03-15T09:16:43.780331Z","iopub.status.idle":"2025-03-15T09:16:43.785691Z","shell.execute_reply.started":"2025-03-15T09:16:43.780303Z","shell.execute_reply":"2025-03-15T09:16:43.784821Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def one_hot_encode(arr, n_labels):\n    \n    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n    \n    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n    \n    one_hot = one_hot.reshape((*arr.shape, n_labels))\n    \n    return one_hot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:16:46.017257Z","iopub.execute_input":"2025-03-15T09:16:46.017537Z","iopub.status.idle":"2025-03-15T09:16:46.021848Z","shell.execute_reply.started":"2025-03-15T09:16:46.017514Z","shell.execute_reply":"2025-03-15T09:16:46.020894Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class LSTM(nn.Module):\n    \n    def __init__(self, chars, device, n_hidden=256, n_layers=2, drop_prob=0.5, bidirectional=False):\n        super().__init__()\n        \n        self.device = device\n        \n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        \n\n        self.n_chars = len(chars)\n        self.int2char = dict(enumerate(chars))\n        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n        \n\n        self.lstm = nn.LSTM(self.n_chars, n_hidden, n_layers, \n                            dropout=drop_prob, batch_first=True, bidirectional=bidirectional)\n        \n        self.dropout = nn.Dropout(drop_prob)\n        \n        self.fc = nn.Linear(n_hidden, self.n_chars)\n        \n    def forward(self, x, hidden):\n        ''' Forward pass through the network. \n            The inputs are x, and the hidden & cell state in a tuple. '''\n        \n\n        r_output, hidden = self.lstm(x, hidden)\n        \n        out = self.dropout(r_output)\n\n        out = out.contiguous().view(-1, self.n_hidden)\n        \n        out = self.fc(out)\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size=1):\n        ''' Initializes hidden state '''\n\n        weight = next(self.parameters()).data\n\n        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n        \n        return hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:55:55.189922Z","iopub.execute_input":"2025-03-15T09:55:55.190254Z","iopub.status.idle":"2025-03-15T09:55:55.197228Z","shell.execute_reply.started":"2025-03-15T09:55:55.190229Z","shell.execute_reply":"2025-03-15T09:55:55.196394Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Declaring the train method\ndef train(model, data, device, optimizer, criterion, epochs=10, batch_size=10,\n          seq_length=50, clip=5):\n    model.train()\n\n    for epoch in range(epochs):\n        h = model.init_hidden(batch_size)\n        total_loss = 0\n        for x, y in get_batches(data, batch_size, seq_length):\n            x = one_hot_encode(x, model.n_chars)\n            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            model.zero_grad()\n\n            output, h = model(inputs, h)\n            h = (h[0].detach(), h[1].detach())\n\n\n\n            loss = criterion(output, targets.view(batch_size*seq_length).long())\n            loss.backward(retain_graph=True)\n\n            total_loss += loss.item()\n\n\n            nn.utils.clip_grad_norm_(model.parameters(), clip)\n            optimizer.step()\n\n        if (epoch + 1) % 100 == 0:\n            checkpoint_path = os.path.join(\"model_checkpoints\", f\"model_epoch_{epoch+1}.pt\")\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': total_loss/batch_size,\n            }, checkpoint_path)\n            print(f\"Checkpoint saved at {checkpoint_path}\")\n\n\n        print(\"Epoch: {}/{}:\".format(epoch + 1, epochs),\n              \"Loss: {:.4f}:\".format(total_loss/batch_size))\n        total_loss = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:55:56.764630Z","iopub.execute_input":"2025-03-15T09:55:56.764984Z","iopub.status.idle":"2025-03-15T09:55:56.771947Z","shell.execute_reply.started":"2025-03-15T09:55:56.764958Z","shell.execute_reply":"2025-03-15T09:55:56.771124Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import os\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ncheckpoint_dir = \"model_checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True) \n\n# Define the model\nn_hidden=512\nn_layers=2\n\nmodel = LSTM(chars, device, n_hidden, n_layers).to(device)\n\n# Declaring the hyperparameters\nbatch_size = 128\nseq_length = 100\nepochs = 600 \n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# train the model\ntrain(model, np.array(encoded_poetry), device, optimizer, criterion, epochs=epochs,\n      batch_size=batch_size, seq_length=seq_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:56:06.407274Z","iopub.execute_input":"2025-03-15T09:56:06.407553Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1/600: Loss: 3.5147:\nEpoch: 2/600: Loss: 2.8579:\nEpoch: 3/600: Loss: 2.6663:\nEpoch: 4/600: Loss: 2.5461:\nEpoch: 5/600: Loss: 2.4465:\nEpoch: 6/600: Loss: 2.3691:\nEpoch: 7/600: Loss: 2.3131:\nEpoch: 8/600: Loss: 2.2659:\nEpoch: 9/600: Loss: 2.2275:\nEpoch: 10/600: Loss: 2.1957:\nEpoch: 11/600: Loss: 2.1681:\nEpoch: 12/600: Loss: 2.1449:\nEpoch: 13/600: Loss: 2.1242:\nEpoch: 14/600: Loss: 2.1064:\nEpoch: 15/600: Loss: 2.0903:\nEpoch: 16/600: Loss: 2.0750:\nEpoch: 17/600: Loss: 2.0598:\nEpoch: 18/600: Loss: 2.0475:\nEpoch: 19/600: Loss: 2.0361:\nEpoch: 20/600: Loss: 2.0236:\nEpoch: 21/600: Loss: 2.0132:\nEpoch: 22/600: Loss: 2.0024:\nEpoch: 23/600: Loss: 1.9936:\nEpoch: 24/600: Loss: 1.9843:\nEpoch: 25/600: Loss: 1.9765:\nEpoch: 26/600: Loss: 1.9687:\nEpoch: 27/600: Loss: 1.9606:\nEpoch: 28/600: Loss: 1.9544:\nEpoch: 29/600: Loss: 1.9472:\nEpoch: 30/600: Loss: 1.9444:\nEpoch: 31/600: Loss: 1.9360:\nEpoch: 32/600: Loss: 1.9303:\nEpoch: 33/600: Loss: 1.9253:\nEpoch: 34/600: Loss: 1.9188:\nEpoch: 35/600: Loss: 1.9149:\nEpoch: 36/600: Loss: 1.9095:\nEpoch: 37/600: Loss: 1.9038:\nEpoch: 38/600: Loss: 1.8976:\nEpoch: 39/600: Loss: 1.8932:\nEpoch: 40/600: Loss: 1.8888:\nEpoch: 41/600: Loss: 1.8847:\nEpoch: 42/600: Loss: 1.8789:\nEpoch: 43/600: Loss: 1.8743:\nEpoch: 44/600: Loss: 1.8709:\nEpoch: 45/600: Loss: 1.8705:\nEpoch: 46/600: Loss: 1.8796:\nEpoch: 47/600: Loss: 1.8696:\nEpoch: 48/600: Loss: 1.8585:\nEpoch: 49/600: Loss: 1.8541:\nEpoch: 50/600: Loss: 1.8498:\nEpoch: 51/600: Loss: 1.8456:\nEpoch: 52/600: Loss: 1.8421:\nEpoch: 53/600: Loss: 1.8397:\nEpoch: 54/600: Loss: 1.8364:\nEpoch: 55/600: Loss: 1.8334:\nEpoch: 56/600: Loss: 1.8302:\nEpoch: 57/600: Loss: 1.8278:\nEpoch: 58/600: Loss: 1.8243:\nEpoch: 59/600: Loss: 1.8213:\nEpoch: 60/600: Loss: 1.8193:\nEpoch: 61/600: Loss: 1.8194:\nEpoch: 62/600: Loss: 1.8131:\nEpoch: 63/600: Loss: 1.8098:\nEpoch: 64/600: Loss: 1.8067:\nEpoch: 65/600: Loss: 1.8036:\nEpoch: 66/600: Loss: 1.8012:\nEpoch: 67/600: Loss: 1.7972:\nEpoch: 68/600: Loss: 1.7969:\nEpoch: 69/600: Loss: 1.7930:\nEpoch: 70/600: Loss: 1.7900:\nEpoch: 71/600: Loss: 1.7870:\nEpoch: 72/600: Loss: 1.7881:\nEpoch: 73/600: Loss: 1.7842:\nEpoch: 74/600: Loss: 1.7811:\nEpoch: 75/600: Loss: 1.7785:\nEpoch: 76/600: Loss: 1.7762:\nEpoch: 77/600: Loss: 1.7722:\nEpoch: 78/600: Loss: 1.7711:\nEpoch: 79/600: Loss: 1.7682:\nEpoch: 80/600: Loss: 1.7670:\nEpoch: 81/600: Loss: 1.7646:\nEpoch: 82/600: Loss: 1.7626:\nEpoch: 83/600: Loss: 1.7595:\nEpoch: 84/600: Loss: 1.7564:\nEpoch: 85/600: Loss: 1.7541:\nEpoch: 86/600: Loss: 1.7573:\nEpoch: 87/600: Loss: 1.7542:\nEpoch: 88/600: Loss: 1.7491:\nEpoch: 89/600: Loss: 1.7461:\nEpoch: 90/600: Loss: 1.7432:\nEpoch: 91/600: Loss: 1.7420:\nEpoch: 92/600: Loss: 1.7392:\nEpoch: 93/600: Loss: 1.7382:\nEpoch: 94/600: Loss: 1.7368:\nEpoch: 95/600: Loss: 1.7337:\nEpoch: 96/600: Loss: 1.7324:\nEpoch: 97/600: Loss: 1.7304:\nEpoch: 98/600: Loss: 1.7283:\nEpoch: 99/600: Loss: 1.7254:\nCheckpoint saved at model_checkpoints/model_epoch_100.pt\nEpoch: 100/600: Loss: 1.7239:\nEpoch: 101/600: Loss: 1.7217:\nEpoch: 102/600: Loss: 1.7195:\nEpoch: 103/600: Loss: 1.7177:\nEpoch: 104/600: Loss: 1.7142:\nEpoch: 105/600: Loss: 1.7137:\nEpoch: 106/600: Loss: 1.7138:\nEpoch: 107/600: Loss: 1.7110:\nEpoch: 108/600: Loss: 1.7090:\nEpoch: 109/600: Loss: 1.7064:\nEpoch: 110/600: Loss: 1.7051:\nEpoch: 111/600: Loss: 1.7281:\nEpoch: 112/600: Loss: 1.7083:\nEpoch: 113/600: Loss: 1.7013:\nEpoch: 114/600: Loss: 1.6989:\nEpoch: 115/600: Loss: 1.6964:\nEpoch: 116/600: Loss: 1.6947:\nEpoch: 117/600: Loss: 1.6923:\nEpoch: 118/600: Loss: 1.6911:\nEpoch: 119/600: Loss: 1.6891:\nEpoch: 120/600: Loss: 1.6879:\nEpoch: 121/600: Loss: 1.6859:\nEpoch: 122/600: Loss: 1.6830:\nEpoch: 123/600: Loss: 1.6821:\nEpoch: 124/600: Loss: 1.6803:\nEpoch: 125/600: Loss: 1.6796:\nEpoch: 126/600: Loss: 1.6783:\nEpoch: 127/600: Loss: 1.6753:\nEpoch: 128/600: Loss: 1.6742:\nEpoch: 129/600: Loss: 1.6731:\nEpoch: 130/600: Loss: 1.6708:\nEpoch: 131/600: Loss: 1.6703:\nEpoch: 132/600: Loss: 1.6671:\nEpoch: 133/600: Loss: 1.6660:\nEpoch: 134/600: Loss: 1.6649:\nEpoch: 135/600: Loss: 1.6646:\nEpoch: 136/600: Loss: 1.6621:\nEpoch: 137/600: Loss: 1.6613:\nEpoch: 138/600: Loss: 1.6603:\nEpoch: 139/600: Loss: 1.6582:\nEpoch: 140/600: Loss: 1.6573:\nEpoch: 141/600: Loss: 1.6557:\nEpoch: 142/600: Loss: 1.6545:\nEpoch: 143/600: Loss: 1.6530:\nEpoch: 144/600: Loss: 1.6520:\nEpoch: 145/600: Loss: 1.6509:\nEpoch: 146/600: Loss: 1.6518:\nEpoch: 147/600: Loss: 1.6493:\nEpoch: 148/600: Loss: 1.6478:\nEpoch: 149/600: Loss: 1.6465:\nEpoch: 150/600: Loss: 1.6446:\nEpoch: 151/600: Loss: 1.6433:\nEpoch: 152/600: Loss: 1.6438:\nEpoch: 153/600: Loss: 1.6415:\nEpoch: 154/600: Loss: 1.6402:\nEpoch: 155/600: Loss: 1.6387:\nEpoch: 156/600: Loss: 1.6378:\nEpoch: 157/600: Loss: 1.6365:\nEpoch: 158/600: Loss: 1.6359:\nEpoch: 159/600: Loss: 1.6345:\nEpoch: 160/600: Loss: 1.6373:\nEpoch: 161/600: Loss: 1.6331:\nEpoch: 162/600: Loss: 1.6360:\nEpoch: 163/600: Loss: 1.6306:\nEpoch: 164/600: Loss: 1.6284:\nEpoch: 165/600: Loss: 1.6284:\nEpoch: 166/600: Loss: 1.6256:\nEpoch: 167/600: Loss: 1.6254:\nEpoch: 168/600: Loss: 1.6237:\nEpoch: 169/600: Loss: 1.6231:\nEpoch: 170/600: Loss: 1.6219:\nEpoch: 171/600: Loss: 1.6214:\nEpoch: 172/600: Loss: 1.6197:\nEpoch: 173/600: Loss: 1.6187:\nEpoch: 174/600: Loss: 1.6170:\nEpoch: 175/600: Loss: 1.6166:\nEpoch: 176/600: Loss: 1.6149:\nEpoch: 177/600: Loss: 1.6136:\nEpoch: 178/600: Loss: 1.6133:\nEpoch: 179/600: Loss: 1.6112:\nEpoch: 180/600: Loss: 1.6108:\nEpoch: 181/600: Loss: 1.6096:\nEpoch: 182/600: Loss: 1.6080:\nEpoch: 183/600: Loss: 1.6063:\nEpoch: 184/600: Loss: 1.6069:\nEpoch: 185/600: Loss: 1.6046:\nEpoch: 186/600: Loss: 1.6041:\nEpoch: 187/600: Loss: 1.6040:\nEpoch: 188/600: Loss: 1.6447:\nEpoch: 189/600: Loss: 1.6085:\nEpoch: 190/600: Loss: 1.6028:\nEpoch: 191/600: Loss: 1.6018:\nEpoch: 192/600: Loss: 1.6000:\nEpoch: 193/600: Loss: 1.5982:\nEpoch: 194/600: Loss: 1.5986:\nEpoch: 195/600: Loss: 1.5977:\nEpoch: 196/600: Loss: 1.5947:\nEpoch: 197/600: Loss: 1.5952:\nEpoch: 198/600: Loss: 1.5937:\nEpoch: 199/600: Loss: 1.5921:\nCheckpoint saved at model_checkpoints/model_epoch_200.pt\nEpoch: 200/600: Loss: 1.5911:\nEpoch: 201/600: Loss: 1.5946:\nEpoch: 202/600: Loss: 1.5903:\nEpoch: 203/600: Loss: 1.5886:\nEpoch: 204/600: Loss: 1.5873:\nEpoch: 205/600: Loss: 1.5876:\nEpoch: 206/600: Loss: 1.5876:\nEpoch: 207/600: Loss: 1.5857:\nEpoch: 208/600: Loss: 1.5845:\nEpoch: 209/600: Loss: 1.5838:\nEpoch: 210/600: Loss: 1.5830:\nEpoch: 211/600: Loss: 1.5825:\nEpoch: 212/600: Loss: 1.5820:\nEpoch: 213/600: Loss: 1.5808:\nEpoch: 214/600: Loss: 1.5800:\nEpoch: 215/600: Loss: 1.5791:\nEpoch: 216/600: Loss: 1.5786:\nEpoch: 217/600: Loss: 1.5776:\nEpoch: 218/600: Loss: 1.5800:\nEpoch: 219/600: Loss: 1.5771:\nEpoch: 220/600: Loss: 1.5789:\nEpoch: 221/600: Loss: 1.5753:\nEpoch: 222/600: Loss: 1.5752:\nEpoch: 223/600: Loss: 1.5731:\nEpoch: 224/600: Loss: 1.5726:\nEpoch: 225/600: Loss: 1.5721:\nEpoch: 226/600: Loss: 1.5725:\nEpoch: 227/600: Loss: 1.5709:\nEpoch: 228/600: Loss: 1.5705:\nEpoch: 229/600: Loss: 1.5686:\nEpoch: 230/600: Loss: 1.5702:\nEpoch: 231/600: Loss: 1.5677:\nEpoch: 232/600: Loss: 1.5674:\nEpoch: 233/600: Loss: 1.5661:\nEpoch: 234/600: Loss: 1.5659:\nEpoch: 235/600: Loss: 1.5649:\nEpoch: 236/600: Loss: 1.5654:\nEpoch: 237/600: Loss: 1.5644:\nEpoch: 238/600: Loss: 1.5630:\nEpoch: 239/600: Loss: 1.5628:\nEpoch: 240/600: Loss: 1.5613:\nEpoch: 241/600: Loss: 1.5610:\nEpoch: 242/600: Loss: 1.5614:\nEpoch: 243/600: Loss: 1.5601:\nEpoch: 244/600: Loss: 1.5604:\nEpoch: 245/600: Loss: 1.5624:\nEpoch: 246/600: Loss: 1.5610:\nEpoch: 247/600: Loss: 1.5585:\nEpoch: 248/600: Loss: 1.5566:\nEpoch: 249/600: Loss: 1.5564:\nEpoch: 250/600: Loss: 1.5557:\nEpoch: 251/600: Loss: 1.5557:\nEpoch: 252/600: Loss: 1.5546:\nEpoch: 253/600: Loss: 1.5544:\nEpoch: 254/600: Loss: 1.5537:\nEpoch: 255/600: Loss: 1.5528:\nEpoch: 256/600: Loss: 1.5526:\nEpoch: 257/600: Loss: 1.5813:\nEpoch: 258/600: Loss: 1.5551:\nEpoch: 259/600: Loss: 1.5554:\nEpoch: 260/600: Loss: 1.5501:\nEpoch: 261/600: Loss: 1.5507:\nEpoch: 262/600: Loss: 1.5500:\nEpoch: 263/600: Loss: 1.5484:\nEpoch: 264/600: Loss: 1.5490:\nEpoch: 265/600: Loss: 1.5474:\nEpoch: 266/600: Loss: 1.5483:\nEpoch: 267/600: Loss: 1.5454:\nEpoch: 268/600: Loss: 1.5455:\nEpoch: 269/600: Loss: 1.5446:\nEpoch: 270/600: Loss: 1.5447:\nEpoch: 271/600: Loss: 1.5442:\nEpoch: 272/600: Loss: 1.5430:\nEpoch: 273/600: Loss: 1.5432:\nEpoch: 274/600: Loss: 1.5405:\nEpoch: 275/600: Loss: 1.5414:\nEpoch: 276/600: Loss: 1.5411:\nEpoch: 277/600: Loss: 1.5408:\nEpoch: 278/600: Loss: 1.5401:\nEpoch: 279/600: Loss: 1.5402:\nEpoch: 280/600: Loss: 1.5394:\nEpoch: 281/600: Loss: 1.5397:\nEpoch: 282/600: Loss: 1.5393:\nEpoch: 283/600: Loss: 1.5369:\nEpoch: 284/600: Loss: 1.5375:\nEpoch: 285/600: Loss: 1.5379:\nEpoch: 286/600: Loss: 1.5367:\nEpoch: 287/600: Loss: 1.5347:\nEpoch: 288/600: Loss: 1.5354:\nEpoch: 289/600: Loss: 1.5325:\nEpoch: 290/600: Loss: 1.5339:\nEpoch: 291/600: Loss: 1.5402:\nEpoch: 292/600: Loss: 1.5330:\nEpoch: 293/600: Loss: 1.5328:\nEpoch: 294/600: Loss: 1.5318:\nEpoch: 295/600: Loss: 1.5314:\nEpoch: 296/600: Loss: 1.5313:\nEpoch: 297/600: Loss: 1.5293:\nEpoch: 298/600: Loss: 1.5293:\nEpoch: 299/600: Loss: 1.5284:\nCheckpoint saved at model_checkpoints/model_epoch_300.pt\nEpoch: 300/600: Loss: 1.5290:\nEpoch: 301/600: Loss: 1.5290:\nEpoch: 302/600: Loss: 1.5275:\nEpoch: 303/600: Loss: 1.5282:\nEpoch: 304/600: Loss: 1.5263:\nEpoch: 305/600: Loss: 1.5252:\nEpoch: 306/600: Loss: 1.5257:\nEpoch: 307/600: Loss: 1.5247:\nEpoch: 308/600: Loss: 1.5241:\nEpoch: 309/600: Loss: 1.5251:\nEpoch: 310/600: Loss: 1.5259:\nEpoch: 311/600: Loss: 1.5253:\nEpoch: 312/600: Loss: 1.5237:\nEpoch: 313/600: Loss: 1.5228:\nEpoch: 314/600: Loss: 1.5215:\nEpoch: 315/600: Loss: 1.5213:\nEpoch: 316/600: Loss: 1.5222:\nEpoch: 317/600: Loss: 1.5197:\nEpoch: 318/600: Loss: 1.5212:\nEpoch: 319/600: Loss: 1.5188:\nEpoch: 320/600: Loss: 1.5193:\nEpoch: 321/600: Loss: 1.5205:\nEpoch: 322/600: Loss: 1.5179:\nEpoch: 323/600: Loss: 1.5171:\nEpoch: 324/600: Loss: 1.5173:\nEpoch: 325/600: Loss: 1.5184:\nEpoch: 326/600: Loss: 1.5177:\nEpoch: 327/600: Loss: 1.5167:\nEpoch: 328/600: Loss: 1.5172:\nEpoch: 329/600: Loss: 1.5163:\nEpoch: 330/600: Loss: 1.5160:\nEpoch: 331/600: Loss: 1.5139:\nEpoch: 332/600: Loss: 1.5141:\nEpoch: 333/600: Loss: 1.5134:\nEpoch: 334/600: Loss: 1.5127:\nEpoch: 335/600: Loss: 1.5133:\nEpoch: 336/600: Loss: 1.5128:\nEpoch: 337/600: Loss: 1.5110:\nEpoch: 338/600: Loss: 1.5118:\nEpoch: 339/600: Loss: 1.5119:\nEpoch: 340/600: Loss: 1.5096:\nEpoch: 341/600: Loss: 1.5108:\nEpoch: 342/600: Loss: 1.5109:\nEpoch: 343/600: Loss: 1.5101:\nEpoch: 344/600: Loss: 1.5086:\nEpoch: 345/600: Loss: 1.5091:\nEpoch: 346/600: Loss: 1.5072:\nEpoch: 347/600: Loss: 1.5090:\nEpoch: 348/600: Loss: 1.5080:\nEpoch: 349/600: Loss: 1.5083:\nEpoch: 350/600: Loss: 1.5087:\nEpoch: 351/600: Loss: 1.5060:\nEpoch: 352/600: Loss: 1.5082:\nEpoch: 353/600: Loss: 1.5053:\nEpoch: 354/600: Loss: 1.5059:\nEpoch: 355/600: Loss: 1.5052:\nEpoch: 356/600: Loss: 1.5035:\nEpoch: 357/600: Loss: 1.5037:\nEpoch: 358/600: Loss: 1.5033:\nEpoch: 359/600: Loss: 1.5286:\nEpoch: 360/600: Loss: 1.5062:\nEpoch: 361/600: Loss: 1.5039:\nEpoch: 362/600: Loss: 1.5022:\nEpoch: 363/600: Loss: 1.5018:\nEpoch: 364/600: Loss: 1.4994:\nEpoch: 365/600: Loss: 1.4994:\nEpoch: 366/600: Loss: 1.4995:\nEpoch: 367/600: Loss: 1.4991:\nEpoch: 368/600: Loss: 1.4992:\nEpoch: 369/600: Loss: 1.4975:\nEpoch: 370/600: Loss: 1.4967:\nEpoch: 371/600: Loss: 1.4982:\nEpoch: 372/600: Loss: 1.4964:\nEpoch: 373/600: Loss: 1.5014:\nEpoch: 374/600: Loss: 1.4973:\nEpoch: 375/600: Loss: 1.4968:\nEpoch: 376/600: Loss: 1.4973:\nEpoch: 377/600: Loss: 1.4952:\nEpoch: 378/600: Loss: 1.4948:\nEpoch: 379/600: Loss: 1.4937:\nEpoch: 380/600: Loss: 1.4934:\nEpoch: 381/600: Loss: 1.4942:\nEpoch: 382/600: Loss: 1.5012:\nEpoch: 383/600: Loss: 1.4951:\nEpoch: 384/600: Loss: 1.4934:\nEpoch: 385/600: Loss: 1.4914:\nEpoch: 386/600: Loss: 1.4919:\nEpoch: 387/600: Loss: 1.4922:\nEpoch: 388/600: Loss: 1.4933:\nEpoch: 389/600: Loss: 1.4905:\nEpoch: 390/600: Loss: 1.4890:\nEpoch: 391/600: Loss: 1.4903:\nEpoch: 392/600: Loss: 1.4912:\nEpoch: 393/600: Loss: 1.4903:\nEpoch: 394/600: Loss: 1.4885:\nEpoch: 395/600: Loss: 1.4891:\nEpoch: 396/600: Loss: 1.4888:\nEpoch: 397/600: Loss: 1.4879:\nEpoch: 398/600: Loss: 1.4864:\nEpoch: 399/600: Loss: 1.4862:\nCheckpoint saved at model_checkpoints/model_epoch_400.pt\nEpoch: 400/600: Loss: 1.4853:\nEpoch: 401/600: Loss: 1.4852:\nEpoch: 402/600: Loss: 1.4853:\nEpoch: 403/600: Loss: 1.4857:\nEpoch: 404/600: Loss: 1.4842:\nEpoch: 405/600: Loss: 1.5297:\nEpoch: 406/600: Loss: 1.4942:\nEpoch: 407/600: Loss: 1.4863:\nEpoch: 408/600: Loss: 1.4841:\nEpoch: 409/600: Loss: 1.4833:\nEpoch: 410/600: Loss: 1.4826:\nEpoch: 411/600: Loss: 1.4823:\nEpoch: 412/600: Loss: 1.4805:\nEpoch: 413/600: Loss: 1.4811:\nEpoch: 414/600: Loss: 1.4806:\nEpoch: 415/600: Loss: 1.4817:\nEpoch: 416/600: Loss: 1.4803:\nEpoch: 417/600: Loss: 1.4799:\nEpoch: 418/600: Loss: 1.4796:\nEpoch: 419/600: Loss: 1.4792:\nEpoch: 420/600: Loss: 1.4777:\nEpoch: 421/600: Loss: 1.4781:\nEpoch: 422/600: Loss: 1.4794:\nEpoch: 423/600: Loss: 1.4774:\nEpoch: 424/600: Loss: 1.4780:\nEpoch: 425/600: Loss: 1.4775:\nEpoch: 426/600: Loss: 1.4779:\nEpoch: 427/600: Loss: 1.4773:\nEpoch: 428/600: Loss: 1.4772:\nEpoch: 429/600: Loss: 1.4773:\nEpoch: 430/600: Loss: 1.4763:\nEpoch: 431/600: Loss: 1.4764:\nEpoch: 432/600: Loss: 1.4760:\nEpoch: 433/600: Loss: 1.4750:\nEpoch: 434/600: Loss: 1.4789:\nEpoch: 435/600: Loss: 1.4767:\nEpoch: 436/600: Loss: 1.4739:\nEpoch: 437/600: Loss: 1.4743:\nEpoch: 438/600: Loss: 1.4737:\nEpoch: 439/600: Loss: 1.4732:\nEpoch: 440/600: Loss: 1.4750:\nEpoch: 441/600: Loss: 1.4730:\nEpoch: 442/600: Loss: 1.4725:\nEpoch: 443/600: Loss: 1.4717:\nEpoch: 444/600: Loss: 1.4709:\nEpoch: 445/600: Loss: 1.4714:\nEpoch: 446/600: Loss: 1.4712:\nEpoch: 447/600: Loss: 1.4712:\nEpoch: 448/600: Loss: 1.4705:\nEpoch: 449/600: Loss: 1.4700:\nEpoch: 450/600: Loss: 1.4702:\nEpoch: 451/600: Loss: 1.4692:\nEpoch: 452/600: Loss: 1.4686:\nEpoch: 465/600: Loss: 1.4660:\nEpoch: 466/600: Loss: 1.4657:\nEpoch: 467/600: Loss: 1.4640:\nEpoch: 468/600: Loss: 1.4644:\nEpoch: 469/600: Loss: 1.4650:\nEpoch: 470/600: Loss: 1.4699:\nEpoch: 471/600: Loss: 1.4637:\nEpoch: 472/600: Loss: 1.4623:\nEpoch: 473/600: Loss: 1.4622:\nEpoch: 474/600: Loss: 1.4617:\nEpoch: 475/600: Loss: 1.4616:\nEpoch: 476/600: Loss: 1.4607:\nEpoch: 477/600: Loss: 1.4613:\nEpoch: 478/600: Loss: 1.4598:\nEpoch: 479/600: Loss: 1.4608:\nEpoch: 480/600: Loss: 1.4607:\nEpoch: 481/600: Loss: 1.4602:\nEpoch: 482/600: Loss: 1.4606:\nEpoch: 483/600: Loss: 1.4582:\nEpoch: 484/600: Loss: 1.4598:\nEpoch: 485/600: Loss: 1.4582:\nEpoch: 486/600: Loss: 1.4593:\nEpoch: 487/600: Loss: 1.4591:\nEpoch: 488/600: Loss: 1.4585:\nEpoch: 489/600: Loss: 1.4582:\nEpoch: 490/600: Loss: 1.4582:\nEpoch: 491/600: Loss: 1.4566:\nEpoch: 492/600: Loss: 1.4574:\nEpoch: 493/600: Loss: 1.4659:\nEpoch: 494/600: Loss: 1.4665:\nEpoch: 495/600: Loss: 1.4574:\nEpoch: 496/600: Loss: 1.4561:\nEpoch: 497/600: Loss: 1.4550:\nEpoch: 498/600: Loss: 1.4554:\nEpoch: 499/600: Loss: 1.4547:\nCheckpoint saved at model_checkpoints/model_epoch_500.pt\nEpoch: 500/600: Loss: 1.4551:\nEpoch: 501/600: Loss: 1.4542:\nEpoch: 502/600: Loss: 1.4558:\nEpoch: 503/600: Loss: 1.4541:\nEpoch: 504/600: Loss: 1.4543:\nEpoch: 505/600: Loss: 1.4546:\nEpoch: 506/600: Loss: 1.4534:\nEpoch: 507/600: Loss: 1.4521:\nEpoch: 508/600: Loss: 1.4532:\nEpoch: 509/600: Loss: 1.4533:\nEpoch: 510/600: Loss: 1.4527:\nEpoch: 511/600: Loss: 1.4532:\nEpoch: 512/600: Loss: 1.4530:\nEpoch: 513/600: Loss: 1.4525:\nEpoch: 514/600: Loss: 1.4509:\nEpoch: 515/600: Loss: 1.4514:\nEpoch: 516/600: Loss: 1.4509:\nEpoch: 517/600: Loss: 1.4510:\nEpoch: 518/600: Loss: 1.4509:\nEpoch: 519/600: Loss: 1.4499:\nEpoch: 520/600: Loss: 1.4500:\nEpoch: 521/600: Loss: 1.4523:\nEpoch: 522/600: Loss: 1.4721:\nEpoch: 523/600: Loss: 1.5028:\nEpoch: 524/600: Loss: 1.4593:\nEpoch: 525/600: Loss: 1.4532:\nEpoch: 526/600: Loss: 1.4580:\nEpoch: 527/600: Loss: 1.4511:\nEpoch: 528/600: Loss: 1.4499:\nEpoch: 529/600: Loss: 1.4496:\nEpoch: 530/600: Loss: 1.4480:\nEpoch: 531/600: Loss: 1.4469:\nEpoch: 532/600: Loss: 1.4482:\nEpoch: 533/600: Loss: 1.4479:\nEpoch: 534/600: Loss: 1.4467:\nEpoch: 535/600: Loss: 1.4460:\nEpoch: 536/600: Loss: 1.4471:\nEpoch: 537/600: Loss: 1.4468:\nEpoch: 538/600: Loss: 1.4460:\nEpoch: 539/600: Loss: 1.4447:\nEpoch: 540/600: Loss: 1.4452:\nEpoch: 541/600: Loss: 1.4465:\nEpoch: 542/600: Loss: 1.4442:\nEpoch: 543/600: Loss: 1.4446:\nEpoch: 544/600: Loss: 1.4438:\nEpoch: 545/600: Loss: 1.4455:\nEpoch: 546/600: Loss: 1.4453:\nEpoch: 547/600: Loss: 1.4443:\nEpoch: 548/600: Loss: 1.4441:\nEpoch: 549/600: Loss: 1.4431:\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def predict(model, char, device, h=None, top_k=5):\n        ''' Given a character & hidden state, predict the next character.\n            Returns the predicted character and the hidden state.\n        '''\n\n        # tensor inputs\n        x = np.array([[model.char2int[char]]])\n        x = one_hot_encode(x, model.n_chars)\n        inputs = torch.from_numpy(x).to(device)\n\n        with torch.no_grad():\n            # get the output of the model\n            out, h = model(inputs, h)\n\n            # get the character probabilities\n            # move to cpu for further processing with numpy etc.\n            p = F.softmax(out, dim=1).data.cpu()\n\n            # get the top characters with highest likelihood\n            p, top_ch = p.topk(top_k)\n            top_ch = top_ch.numpy().squeeze()\n\n            # select the likely next character with some element of randomness\n            # for more variability\n            p = p.numpy().squeeze()\n            char = np.random.choice(top_ch, p=p/p.sum())\n\n        # return the encoded value of the predicted char and the hidden state\n        return model.int2char[char], h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:17:45.568348Z","iopub.execute_input":"2025-03-15T11:17:45.568621Z","iopub.status.idle":"2025-03-15T11:17:45.574327Z","shell.execute_reply.started":"2025-03-15T11:17:45.568599Z","shell.execute_reply":"2025-03-15T11:17:45.573444Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def sample(model, size, device, prime='A', top_k=None):\n    # method to generate new text based on a \"prime\"/initial sequence.\n    # Basically, the outer loop convenience function that calls the above\n    # defined predict method.\n    model.eval() # eval mode\n\n    # Calculate model for the initial prime characters\n    chars = [ch for ch in prime]\n    with torch.no_grad():\n        # initialize hidden with 0 in the beginning. Set our batch size to 1\n        # as we wish to generate one sequence only.\n        h = model.init_hidden(batch_size=1)\n        for ch in prime:\n            char, h = predict(model, ch, device, h=h, top_k=top_k)\n\n        # append the characters to the sequence\n        chars.append(char)\n\n        # Now pass in the previous/last character and get a new one\n        # Repeat this process for the desired length of the sequence to be\n        # generated\n        for ii in range(size):\n            char, h = predict(model, chars[-1], device, h=h, top_k=top_k)\n            chars.append(char)\n\n    return ''.join(chars)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:17:47.411046Z","iopub.execute_input":"2025-03-15T11:17:47.411350Z","iopub.status.idle":"2025-03-15T11:17:47.416623Z","shell.execute_reply.started":"2025-03-15T11:17:47.411322Z","shell.execute_reply":"2025-03-15T11:17:47.415682Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"checkpoint_path = \"model_checkpoints/model_epoch_600.pt\"\ncheckpoint = torch.load(checkpoint_path)\n\nmodel_loaded = LSTM(chars, device, n_hidden, n_layers).to(device)\n\n\nmodel_loaded.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:18:49.826296Z","iopub.execute_input":"2025-03-15T11:18:49.826594Z","iopub.status.idle":"2025-03-15T11:18:49.885693Z","shell.execute_reply.started":"2025-03-15T11:18:49.826568Z","shell.execute_reply":"2025-03-15T11:18:49.884656Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-32-eb5e7396ccb2>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path)\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"print(sample(model_loaded, 1000, device, prime='соняшник у полі', top_k=5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:18:54.790204Z","iopub.execute_input":"2025-03-15T11:18:54.790496Z","iopub.status.idle":"2025-03-15T11:18:55.360293Z","shell.execute_reply.started":"2025-03-15T11:18:54.790474Z","shell.execute_reply":"2025-03-15T11:18:55.359391Z"}},"outputs":[{"name":"stdout","text":"соняшник у полі співає:\nсерце, стане і своє, і за мене вовчий\nтакої великі плоди. сидів у полкаві.\nти не славний, не просто загадай,\nне винен така прокляте й на мале.\nа я з подаленя серце співанем.\nвсе наталює вітер в палети.\nз тебе віти святого насарти,\nпри тебе бачить від серденька підвів.\nсаранські серця з ваші пахощів\nв серці від них був на світі несуть.\nтам вона повівала від себе з дому…\nа все ж дуже вираз тобі спала.\nпро те, що нас не проститься в світі,\nяк потопа і в сонці, дола світля!\nми страшніше, якби винула вона,\nненаче в своїй відьма стільки сила.\nпроклята буде в морозок принесла,\nна світі ж прості вічність принусеш.\nта в тебе ніччю не вмію, загубив,\nі тоді вітер підкупив в дорогу.\nтільки так не те сивий навіть мала,\nяк встав світанку і все сміх мене.\nа всі досидять наш достого невідомо,\nскрізь від своїх вариних не встидаю,\nі все забув сестри при мій драмину,\nзасвітивсь навколо в невідрожі,\nпро сліпим скорінням над водою,\nяк повідряному пора не встане.\nі навіть в світі живе потоків,\nне пл\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}